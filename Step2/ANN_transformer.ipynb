{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保每次运行时，随机数种子是不同的\n",
    "random_seed = int(time.time())\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "DOWNLOAD_PATH = 'data'\n",
    "OUTPUT_PATH='vit_mnist_print.txt'\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000\n",
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f37afce2a40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_environment(images, labels, e):\n",
    "    def torch_bernoulli(p, size):\n",
    "        return (torch.rand(size) < p).float()\n",
    "    def torch_xor(a, b):\n",
    "        return (a - b).abs()  # Assumes both inputs are either 0 or 1\n",
    "    # 2x subsample for computational convenience\n",
    "    images = images.reshape((-1, 28, 28))[:, ::2, ::2]\n",
    "    # Assign a binary label based on the digit; flip label with probability 0.25\n",
    "    labels = (labels < 5).float()\n",
    "    labels = torch_xor(labels, torch_bernoulli(0.25, len(labels)))\n",
    "    # Assign a color based on the label; flip the color with probability e\n",
    "    colors = torch_xor(labels, torch_bernoulli(e, len(labels)))\n",
    "    # Apply the color to the image by zeroing out the other color channel\n",
    "    images = torch.stack([images, images], dim=1)\n",
    "    # images[torch.tensor(range(len(images))), (1 - colors).long(), :, :] *= 0\n",
    "    images = images.clone()\n",
    "    images[torch.tensor(range(len(images))), (1 - colors).long(), :, :] = 0\n",
    "    return {\n",
    "        'images': (images.float() / 255.),\n",
    "        'labels': labels[:, None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印结果以及输出到文件中\n",
    "def print_info(string,file='vit_print.txt'):\n",
    "    print(string)\n",
    "    with open(file,'a') as f:\n",
    "        f.write(string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#残差模块，放在每个前馈网络和注意力之后\n",
    "class Residual(nn.Module): # 通过连接层或者补充，保证fn输出和x是同维度的\n",
    "    def __init__(self, fn): # 带function参数的Module，都是嵌套的Module\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layernorm归一化,放在多头注意力层和激活函数层。用绝对位置编码的BERT，layernorm用来自身通道归一化\n",
    "class PreNorm(nn.Module): # 先归一化，再用function作用。\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim) # 三维的用dim，四维用[C,H,W]\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#放置多头注意力后，因为在于多头注意力使用的矩阵乘法为线性变换，后面跟上由全连 接网络构成的FeedForward增加非线性结构\n",
    "class FeedForward(nn.Module): # 非线性前馈，保持dim维不变\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多头注意力层，多个自注意力连起来。使用qkv计算\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
    "\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, float('-inf'))\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([]) # ModuleList套ModuleList\n",
    "        for _ in range(depth): # 叠加Attention块\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, mask=mask)\n",
    "            x = ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将图像切割成一个个图像块,组成序列化的数据输入Transformer执行图像分类任务。\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, '报错：图像没有被patch_size完美分割'\n",
    "        num_patches = (image_size // patch_size) # ** 2\n",
    "        patch_dim = 2 * channels * patch_size ** 2 # (P**2 C)：一个patch展平为向量后实际的长度\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) # +1是为了适应cls_token\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim) # 将patch_dim（原图）经过embedding后得到dim维的嵌入向量\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        p = self.patch_size\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "        # print(f\"Shape after rearrange: {x.shape}\")  # 添加打印语句\n",
    "        # print(f\"Shape of patch_to_embedding weight: {self.patch_to_embedding.weight.shape}\")  # 添加打印语句\n",
    "        x = self.patch_to_embedding(x)\n",
    "        # print(f\"Shape after patch_to_embedding: {x.shape}\")  # 添加打印语句\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        # print(f\"Shape of cls_tokens: {cls_tokens.shape}\")  # 添加打印语句\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # print(f\"Shape after concatenation: {x.shape}\")  # 添加打印语句\n",
    "        x = x + self.pos_embedding\n",
    "        # print(\"Shape after adding pos_embedding: \", x.shape)  # 添加打印语句\n",
    "        x = self.transformer(x, mask)\n",
    "        # print(\"Shape after transformer: \", x.shape)  # 添加打印语句\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        # print(\"Shape after to_cls_token: \", x.shape)  # 添加打印语句\n",
    "        y = self.mlp_head(x)\n",
    "        # print(\"Shape after mlp_head: \", y.shape)  # 添加打印语句\n",
    "        \n",
    "                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        target = target.squeeze().long()  # 去掉多余的维度\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print_info('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()), OUTPUT_PATH)\n",
    "            loss_history.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = F.log_softmax(model(data), dim=1)\n",
    "            target = target.squeeze().long()  # 去掉多余的维度\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    print_info('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n',OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化到[-1, 1]之间\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 60000\n",
      "Shape of one image: torch.Size([2, 14, 14])\n",
      "Label of first image: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# 加载 MNIST 数据集\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_dataset = make_environment(train_dataset.data, train_dataset.targets, 0.2)\n",
    "\n",
    "# 打印 train_dataset 的信息\n",
    "print(f\"Number of images: {len(train_dataset['images'])}\")\n",
    "print(f\"Shape of one image: {train_dataset['images'][0].shape}\")\n",
    "print(f\"Label of first image: {train_dataset['labels'][0]}\")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=list(zip(train_dataset['images'], train_dataset['labels'])),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_set = make_environment(test_set.data, test_set.targets, 0.2)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=list(zip(test_set['images'], test_set['labels'])),\n",
    "    # dataset=test_set,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保每次运行时，模型的初始参数是随机的\n",
    "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1, # 模型\n",
    "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n",
      "[    0/60000 (  0%)]  Loss: 2.3299\n",
      "[12800/60000 ( 21%)]  Loss: 0.4979\n",
      "[25600/60000 ( 43%)]  Loss: 0.3921\n",
      "[38400/60000 ( 64%)]  Loss: 0.4473\n",
      "[51200/60000 ( 85%)]  Loss: 0.3940\n",
      "\n",
      "Average test loss: 0.4884  Accuracy: 7914/10000 (79.14%)\n",
      "\n",
      "spend 0.7220174908638001 min\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "[    0/60000 (  0%)]  Loss: 0.4805\n",
      "[12800/60000 ( 21%)]  Loss: 0.4392\n",
      "[25600/60000 ( 43%)]  Loss: 0.5145\n",
      "[38400/60000 ( 64%)]  Loss: 0.5194\n",
      "[51200/60000 ( 85%)]  Loss: 0.4589\n",
      "\n",
      "Average test loss: 0.4728  Accuracy: 7979/10000 (79.79%)\n",
      "\n",
      "spend 0.6903076847394307 min\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "[    0/60000 (  0%)]  Loss: 0.4586\n",
      "[12800/60000 ( 21%)]  Loss: 0.5133\n",
      "[25600/60000 ( 43%)]  Loss: 0.5407\n",
      "[38400/60000 ( 64%)]  Loss: 0.4230\n",
      "[51200/60000 ( 85%)]  Loss: 0.4260\n",
      "\n",
      "Average test loss: 0.4859  Accuracy: 7969/10000 (79.69%)\n",
      "\n",
      "spend 0.6507037440935771 min\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "[    0/60000 (  0%)]  Loss: 0.4854\n",
      "[12800/60000 ( 21%)]  Loss: 0.5090\n",
      "[25600/60000 ( 43%)]  Loss: 0.4201\n",
      "[38400/60000 ( 64%)]  Loss: 0.4160\n",
      "[51200/60000 ( 85%)]  Loss: 0.5126\n",
      "\n",
      "Average test loss: 0.4653  Accuracy: 7979/10000 (79.79%)\n",
      "\n",
      "spend 0.5762852390607198 min\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "[    0/60000 (  0%)]  Loss: 0.4721\n",
      "[12800/60000 ( 21%)]  Loss: 0.4387\n",
      "[25600/60000 ( 43%)]  Loss: 0.4841\n",
      "[38400/60000 ( 64%)]  Loss: 0.4687\n",
      "[51200/60000 ( 85%)]  Loss: 0.4095\n",
      "\n",
      "Average test loss: 0.4563  Accuracy: 7912/10000 (79.12%)\n",
      "\n",
      "spend 0.5734683911005656 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # 记录时间\n",
    "train_loss_history, test_loss_history = [], [] # 精度记录\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    temp_time=time.time()\n",
    "    print_info(f'Epoch: {epoch}\\n',OUTPUT_PATH)\n",
    "        \n",
    "    train_epoch(model, optimizer, train_data_loader, train_loss_history) # 训练一epoch\n",
    "    evaluate(model, test_data_loader, test_loss_history) # 评估\n",
    "    \n",
    "    print_info(f'spend {(time.time()-temp_time)/60} min\\n',OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.21 min \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_info('Execution time:'+ '{:5.2f}'.format((time.time() - start_time)/60) + ' min \\n',OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yny_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
